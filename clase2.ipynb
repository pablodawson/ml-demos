{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definicion de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard PyTorch implementation of VGG. Pretrained imagenet model is used.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.features = nn.Sequential(\n",
    "            # conv1\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True),\n",
    "            \n",
    "            # conv2\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True),\n",
    "\n",
    "            # conv3\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True),\n",
    "\n",
    "            # conv4\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True),\n",
    "\n",
    "            # conv5\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 1000)\n",
    "        )\n",
    "\n",
    "        # We need these for MaxUnpool operation\n",
    "        self.conv_layer_indices = [0, 2, 5, 7, 10, 12, 14, 17, 19, 21, 24, 26, 28]\n",
    "        self.feature_maps = OrderedDict()\n",
    "        self.pool_locs = OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG()\n",
    "temp = torchvision.models.vgg16(pretrained=True)\n",
    "vgg.load_state_dict(temp.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_imagen(imagen):\n",
    "    imagen = imagen.numpy()\n",
    "    imagen = imagen/imagen.max()\n",
    "    imagen = np.clip(imagen, 0, 1)\n",
    "    return Image.fromarray(np.uint8(imagen*255))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagen de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open(\"dog.jpg\")\n",
    "input_image.resize((224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformaciones imagen -> tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    vgg.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vista interna del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar los filtros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visTensor(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n",
    "        n,c,w,h = tensor.shape\n",
    "\n",
    "        if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "        elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "        rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "        grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "        plt.figure( figsize=(nrow,rows) )\n",
    "        img = grid.numpy().transpose((1, 2, 0))\n",
    "        width = 200\n",
    "        height = img.shape[0] * width // img.shape[1]\n",
    "\n",
    "        return Image.fromarray(np.uint8(img*255)).resize((width,height), resample=Image.Resampling.NEAREST)\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciar en capa 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=-1\n",
    "x = input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 31 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Pablo\\ml-demos\\clase2.ipynb Cell 19\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pablo/ml-demos/clase2.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m j\u001b[39m<\u001b[39m\u001b[39mlen\u001b[39m(vgg\u001b[39m.\u001b[39mfeatures):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pablo/ml-demos/clase2.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     j \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Pablo/ml-demos/clase2.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m layer \u001b[39m=\u001b[39m vgg\u001b[39m.\u001b[39mfeatures[j]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pablo/ml-demos/clase2.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, nn\u001b[39m.\u001b[39mMaxPool2d):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pablo/ml-demos/clase2.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     x, location \u001b[39m=\u001b[39m layer(x)\n",
      "File \u001b[1;32mc:\\Users\\Pablo\\miniconda3\\envs\\joint\\lib\\site-packages\\torch\\nn\\modules\\container.py:107\u001b[0m, in \u001b[0;36mSequential.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(OrderedDict(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems())[idx]))\n\u001b[0;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_item_by_idx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_modules\u001b[39m.\u001b[39;49mvalues(), idx)\n",
      "File \u001b[1;32mc:\\Users\\Pablo\\miniconda3\\envs\\joint\\lib\\site-packages\\torch\\nn\\modules\\container.py:98\u001b[0m, in \u001b[0;36mSequential._get_item_by_idx\u001b[1;34m(self, iterator, idx)\u001b[0m\n\u001b[0;32m     96\u001b[0m idx \u001b[39m=\u001b[39m operator\u001b[39m.\u001b[39mindex(idx)\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m-\u001b[39msize \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m idx \u001b[39m<\u001b[39m size:\n\u001b[1;32m---> 98\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mindex \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is out of range\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(idx))\n\u001b[0;32m     99\u001b[0m idx \u001b[39m%\u001b[39m\u001b[39m=\u001b[39m size\n\u001b[0;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(islice(iterator, idx, \u001b[39mNone\u001b[39;00m))\n",
      "\u001b[1;31mIndexError\u001b[0m: index 31 is out of range"
     ]
    }
   ],
   "source": [
    "if j == len(vgg.features) -1:\n",
    "    print(\"Features listas. No seguir.\")\n",
    "with torch.no_grad():\n",
    "\n",
    "    layer = vgg.features[j]\n",
    "\n",
    "    if isinstance(layer, nn.MaxPool2d):\n",
    "        x, location = layer(x)\n",
    "    else:\n",
    "        x = layer(x)\n",
    "    \n",
    "    # Visualizar las convoluciones\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        filtro = layer.weight.data.clone().cpu()\n",
    "        filtroVis = visTensor(filtro, ch=0, allkernels=False)\n",
    "\n",
    "    vis = x[0].cpu()\n",
    "    images = [visualizar_imagen(i) for i in vis]\n",
    "    images = images[:10]\n",
    "    \n",
    "    imgVis = image_grid(images, 2, 5)\n",
    "\n",
    "    print(j)\n",
    "    print(layer)\n",
    "    print(\"Dimension tensor: \", x.shape)\n",
    "    print(\"Dimension filtros: \",filtro.shape)\n",
    "    j += 1\n",
    "\n",
    "imgVis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=200x200>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtroVis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(x.size()[0], -1)\n",
    "x = vgg.classifier(x)\n",
    "output = x\n",
    "\n",
    "# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for layer in vgg.features:\n",
    "        if isinstance(layer, nn.MaxPool2d):\n",
    "            x, location = layer(x)\n",
    "        else:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Visualizar las convoluciones\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            filtro = layer.weight.data.clone().cpu()\n",
    "            visTensor(filtro, ch=0, allkernels=False)\n",
    "\n",
    "        vis = x[0].cpu()\n",
    "        images = [visualizar_imagen(i) for i in vis]\n",
    "        \n",
    "        i = 0\n",
    "        for i, image in enumerate(images):\n",
    "            i += 1\n",
    "            if i > 10:\n",
    "                break\n",
    "            image.save('feature_maps/output_{}.jpg'.format(i))\n",
    "\n",
    "        print(\"\")\n",
    "    \n",
    "    x = x.view(x.size()[0], -1)\n",
    "    x = vgg.classifier(x)\n",
    "    output = x\n",
    "\n",
    "# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
